# -*- coding: utf-8 -*-
"""judgement_final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wL2cIpbB4vAUMix1l_Knz9bXRyXdfISu
"""

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, cross_validate
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.preprocessing import OneHotEncoder, LabelEncoder

justice_df=pd.read_csv('justice.csv')
justice_df.head(5)

justice_df.info()

justice_df['disposition'].value_counts()

justice_df['decision_type'].value_counts()

"""**데이터 전처리**"""

justice_df.dropna(inplace=True)
justice_df.info()
justice_df.reset_index(drop=True, inplace=True)
justice_df.head(5)

#영문자가 아닌 것은 공백으로 대체
import re
justice_df['facts']=justice_df['facts'].apply(lambda x: re.sub("[^a-zA-Z]", " ", x))

justice_target=justice_df['first_party_winner']
justice_fact=justice_df['facts']
justice_encode=justice_df[['decision_type', 'disposition']]

#if true means that the first party won, and if false it means that the second party won.
encoder=LabelEncoder()
labels=encoder.fit_transform(justice_target)
df_target=pd.DataFrame(labels, columns=['first_party_winner'])
display(df_target)

#텍스트 전처리
from nltk import sent_tokenize, word_tokenize
from nltk.stem import WordNetLemmatizer
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
stopwords=nltk.corpus.stopwords.words('english')

lemma=WordNetLemmatizer()

def preprocess(data):
  for i, text in enumerate(data):
    text=str(text)
    sentences=sent_tokenize(text=text)
    filtered_words=[]
    for sentence in sentences:
      words=word_tokenize(sentence)
      for word in words:
        word=word.lower()
        if word not in stopwords:
          lemma.lemmatize(word)
          filtered_words.append(word)
    text=" ".join(filtered_words[1:])
    data[i]=text
  return data

preprocess(justice_fact)
display(justice_fact.head(5))

df=pd.concat([justice_fact, justice_encode, df_target], axis=1)
X=df[['facts', 'decision_type', 'disposition']]
y=df['first_party_winner']
df_backup=df.copy()
df

#피처 간 상관관계가 있는지 확인
df.groupby(['first_party_winner'])['decision_type'].value_counts()

df.groupby(['first_party_winner'])['disposition'].value_counts()
#관계가 있다!! first_party는 어쨌든 재판을 신청한 쪽임!

#피처 벡터화(count 기반)
cnt_vec=CountVectorizer()
X_facts=cnt_vec.fit_transform(df['facts'])

X_facts=X_facts.todense()
df_vec=pd.DataFrame(data=X_facts) #columns=cnt_vec.get_feature_names_out()
df_vec

#피처 벡터화(TF-IDF)
from sklearn.feature_extraction.text import TfidfVectorizer

tfidf_vec=TfidfVectorizer()
X_facts_tf=tfidf_vec.fit_transform(df['facts'])
X_facts_tf=X_facts_tf.todense()
df_vec_tf=pd.DataFrame(data=X_facts_tf)
df_vec_tf

#원-핫 인코딩으로 나머지 피처 2개의 형식 맞춰주기
df_encode1=pd.get_dummies(df['decision_type'])
df_encode2=pd.get_dummies(df['disposition'])
df_encode2

df_last=pd.concat([df_vec, df_encode1, df_encode2, df['first_party_winner']], axis=1)
df_last.columns=df_last.columns.astype(str)
df_last

#tf-idf
df_last_tf=pd.concat([df_vec_tf, df_encode1, df_encode2, df['first_party_winner']], axis=1)
df_last_tf.columns=df_last_tf.columns.astype(str)
df_last_tf

"""**예측 및 평가**"""

X_features=df_last.drop(columns=['first_party_winner'])
y_labels=df_last['first_party_winner']
X_train, X_test, y_train, y_test=train_test_split(X_features, y_labels, test_size=0.2, random_state=156)

X_features_tf=df_last_tf.drop(columns=['first_party_winner'])
y_labels_tf=df_last_tf['first_party_winner']
X_train_tf, X_test_tf, y_train_tf, y_test_tf=train_test_split(X_features_tf, y_labels_tf, test_size=0.2, random_state=156)

def model_train_predict(model, X_train, X_test, y_train):
  model.fit(X_train, y_train)
  preds=model.predict(X_test)

  return preds

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

def get_clf_eval(y_test, pred):
  accuracy=accuracy_score(y_test, pred)
  precision=precision_score(y_test, pred)
  recall=recall_score(y_test, pred)
  f1=f1_score(y_test, pred)

  print('정확도: {0:.4f},정밀도: {1:.4f},재현율: {2:.4f}, F1: {3:.4f} \n'.format(accuracy, precision, recall,f1))

from sklearn.model_selection import GridSearchCV

def param_tuning(model, params, xtrain, ytrain):
  grid_cv=GridSearchCV(model, param_grid=params, cv=3)
  grid_cv.fit(xtrain, ytrain)
  print('최적 하이퍼 파라미터: \n', grid_cv.best_params_)
  print('최고 예측 정확도: {0:.4f}'.format(grid_cv.best_score_))

def verify(model, features, labels):
  scores=cross_val_score(model, features, labels, scoring='accuracy', cv=3)
  print('교차 검증별 정확도: \n', np.round(scores, 4))
  print('평균 검증 정확도:', np.round(np.mean(scores), 4))

"""**LDA를 통한 토픽 모델링**

from sklearn.decomposition import LatentDirichletAllocation

lda=LatentDirichletAllocation(n_components=200, random_state=0)
lda_data=lda.fit_transform(X_train)
print(lda_data)
lda.components_
"""

#lda_data.shape #lda_data는 각 사건이 토픽1, 토픽 2에 할당될 확률을 갖고 있는듯!

#X_train_lda=pd.DataFrame(data=lda_data)
#X_test_lda=pd.DataFrame(data=lda.transform(X_test))

#lda_tf=LatentDirichletAllocation(n_components=2, random_state=0)
#lda_data_tf=lda_tf.fit_transform(X_train_tf)

#X_train_lda_tf=pd.DataFrame(data=lda_data_tf)
#X_test_lda_tf=pd.DataFrame(data=lda_tf.transform(X_test_tf))

#display(X_train_lda)

"""**분류를 통한 예측(지도학습)**"""

#랜덤 포레스트
from sklearn.ensemble import RandomForestClassifier

rf_clf=RandomForestClassifier(random_state=156)
preds=model_train_predict(rf_clf, X_train, X_test, y_train)
get_clf_eval(y_test, preds)

params={
    'max_depth':[8, 16, 24],
    'min_samples_leaf':[1, 6, 12],
    'min_samples_split':[2, 8, 16]
}
param_tuning(rf_clf, params, X_train, y_train)

import seaborn as sns

rf_clf_tf=RandomForestClassifier(random_state=156)
preds_tf=model_train_predict(rf_clf_tf, X_train_tf, X_test_tf, y_train_tf)
get_clf_eval(y_test_tf, preds_tf)

#피처 중요도 확인 ㄷㄷㄷㄷ 존나 중요!!!
importance=rf_clf_tf.feature_importances_
facts_importance=sum(importance[0:18399])
decision_type_importance=sum(importance[18399:18407])
disposition_importance=sum(importance[18407:18416])

importance_df=pd.DataFrame({'features': ['facts', 'decision_type', 'disposition'],
                            'value': [facts_importance, decision_type_importance, disposition_importance]})
sns.barplot(x='features', y='value', data=importance_df)

"""
params={
    'max_depth':[8, 16, 24],
    'min_samples_leaf':[1, 6, 12],
    'min_samples_split':[2, 8, 16]
}
param_tuning(rf_clf_tf, params, X_train_tf, y_train_tf)
"""

#xgboost
from xgboost import XGBClassifier

xgb_wrapper=XGBClassifier(n_estimators=400, learning_rate=0.05, max_depth=3, eval_metric='logloss')
preds=model_train_predict(xgb_wrapper, X_train, X_test, y_train)
get_clf_eval(y_test, preds)

xgb_wrapper_tf=XGBClassifier(n_estimators=400, learning_rate=0.05, max_depth=3, eval_metric='logloss')
preds=model_train_predict(xgb_wrapper_tf, X_train_tf, X_test_tf, y_train_tf)
get_clf_eval(y_test_tf, preds)

#lightgbm

from lightgbm import LGBMClassifier, plot_importance

lgbm_wrapper=LGBMClassifier(n_estimators=400, learning_rate=0.05)
lgbm_preds=model_train_predict(lgbm_wrapper, X_train, X_test, y_train)
get_clf_eval(y_test, lgbm_preds)

lgbm_wrapper_tf=LGBMClassifier(n_estimators=400, learning_rate=0.05)
lgbm_preds_tf=model_train_predict(lgbm_wrapper_tf, X_train_tf, X_test_tf, y_train_tf)
get_clf_eval(y_test_tf, lgbm_preds_tf)

#로지스틱회귀
from sklearn.linear_model import LogisticRegression

lr_clf=LogisticRegression(solver='liblinear', random_state=156)
lr_preds=model_train_predict(lr_clf, X_train, X_test, y_train)
get_clf_eval(y_test, lr_preds)

lr_clf_tf=LogisticRegression(solver='liblinear', random_state=156)
lr_preds_tf=model_train_predict(lr_clf_tf, X_train_tf, X_test_tf, y_train_tf)
get_clf_eval(y_test_tf, lr_preds_tf)

"""**K평균을 통한 문서 군집화(비지도학습)**"""

from sklearn.cluster import KMeans

df_cluster=X_features.copy()
df_cluster.columns=df_cluster.columns.astype(str) #칼럼명이 str여야 한다는 오류 수정 위함
km_cluster=KMeans(n_clusters=2, max_iter=10000, random_state=0)
km_cluster.fit(df_cluster)
cluster_label=km_cluster.labels_

df_cluster['cluster_label']=cluster_label
df_cluster.head(5)

num=0

for i in range(3098):
  if y_labels[i]==cluster_label[i]:
    num+=1

kmeans_accuracy=num*100/3098
print(kmeans_accuracy)

"""**decision_type 피처 없이 도전!**
있을때랑 비교
"""

df_2=pd.concat([df_vec, df_encode2, df['first_party_winner']], axis=1)
df_2.columns=df_2.columns.astype(str)
df_2

X_features_2=df_2.drop(columns=['first_party_winner'])
y_labels_2=df_2['first_party_winner']
X_train_2, X_test_2, y_train_2, y_test_2=train_test_split(X_features_2, y_labels_2, test_size=0.2, random_state=156)

rf_clf_2=RandomForestClassifier(random_state=156)
preds_2=model_train_predict(rf_clf_2, X_train_2, X_test_2, y_train_2)
get_clf_eval(y_test_2, preds_2)

#교차검증까지 해봄
rf_clf_3=RandomForestClassifier(random_state=156)
verify(rf_clf_3, X_features_2, y_labels_2)